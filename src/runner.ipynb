{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1994\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 2281\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1929\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 2076\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1945\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1775\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1971\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 2093\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 974\n",
      "[DataLoader]: initialized successfully\n",
      "[DataLoader]: dataset size -- 1009\n",
      "Begin training...\n",
      "Tue Mar  5 17:07:02 2019\n",
      "\n",
      "\n",
      "Epoch (train): 100 \tAccuracy: 0.5\n",
      "Tue Mar  5 17:07:03 2019\n",
      "3.167240389596195e-08\n",
      "\n",
      "\n",
      "Epoch (train): 200 \tAccuracy: 1.0\n",
      "Tue Mar  5 17:07:03 2019\n",
      "1.0010523441728065e-07\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Model-Agnostic Meta-Learning on mnist\n",
    "\n",
    "https://arxiv.org/pdf/1703.03400.pdf\n",
    "\n",
    "Created on Jan 2019\n",
    "\n",
    "@author: zyf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DataLoader import DataLoader\n",
    "from utils import *\n",
    "\n",
    "mndata = MNIST('../data')\n",
    "images_train, labels_train = mndata.load_training()\n",
    "images_test, labels_test = mndata.load_testing()\n",
    "\n",
    "\n",
    "### Split data ###\n",
    "\n",
    "n_category = 10\n",
    "category_train = [0,1,2,3,4,5,6,7]\n",
    "category_test = [8,9]\n",
    "\n",
    "d = len(images_train[0])\n",
    "images_train = images_train[:20000]\n",
    "labels_train = labels_train[:20000]\n",
    "images_test = images_test[-20000:]\n",
    "labels_test = labels_test[-20000:]\n",
    "\n",
    "images_train_categories = [[] for _ in range(n_category)]\n",
    "images_test_categories = [[] for _ in range(n_category)]\n",
    "\n",
    "for idx in range(len(images_train)):\n",
    "    category = labels_train[idx]\n",
    "    if category in category_train:\n",
    "        images_train_categories[category].append([1]+images_train[idx])\n",
    "\n",
    "for idx in range(len(labels_test)):\n",
    "    category = labels_test[idx]\n",
    "    if category in category_test:\n",
    "        images_test_categories[category].append([1]+images_test[idx])\n",
    "    \n",
    "\n",
    "# initialize dataloader\n",
    "trainLoaders = []\n",
    "testLoaders = []\n",
    "\n",
    "# k_shots = 2\n",
    "# batch_size = 2*k_shots     # mini batch size for training\n",
    "\n",
    "for c in range(n_category):\n",
    "    if c in category_train:\n",
    "        trainLoaders.append(DataLoader(np.array([(i+[c]) for i in images_train_categories[c]])))\n",
    "        trainLoaders[-1].reset(0)\n",
    "    if c in category_test:\n",
    "        testLoaders.append(DataLoader(np.array([(i+[c]) for i in images_test_categories[c]])))\n",
    "        testLoaders[-1].reset(0)\n",
    "\n",
    "\n",
    "\n",
    "# A single episode for meta-learning (either for train or fine-tune)\n",
    "\n",
    "def oneEpisode(is_meta=True, is_train=True):\n",
    "    \n",
    "    global w_meta\n",
    "    w_copy = w_meta + 0  # make copy\n",
    "    w_base = np.zeros((d+1,1))  # baseline parameter initialize\n",
    "    gw_meta_avg, gw_base_avg = 0, 0\n",
    "    \n",
    "    n_task = num_task if is_train else 1\n",
    "    max_iter = 1 if is_train else n_step_tuning\n",
    "    \n",
    "    # determine parameter\n",
    "    dataLoaders = trainLoaders if is_train else testLoaders\n",
    "    ctgy_range = len(category_train) if is_train else len(category_test)\n",
    "    sample_size = batch_size if is_train else 0\n",
    "\n",
    "    task_pool = []\n",
    "    for i_task in range(n_task):\n",
    "        task_pool.append(getSamples(dataLoaders, ctgy_range, sample_size))\n",
    "        \n",
    "    for iter_i in range(max_iter):\n",
    "        \n",
    "        for task_data in task_pool:\n",
    "\n",
    "            inputs_alpha, targets_alpha, inputs_bravo, targets_bravo = task_data\n",
    "            \n",
    "            gw_meta, accu_train_meta, accu_query_meta = getGradient(inputs_alpha, targets_alpha, inputs_bravo, targets_bravo, w_meta, k_shots, learn_rate_alpha, is_train)\n",
    "            gw_meta_avg += gw_meta / n_task\n",
    "            \n",
    "            accu_train_base, accu_query_base = 0, 0\n",
    "\n",
    "            if is_train: continue\n",
    "                \n",
    "            gw_base, accu_train_base, accu_query_base = getGradient(inputs_alpha, targets_alpha, inputs_bravo, targets_bravo, w_base, k_shots, learn_rate_alpha, is_train)\n",
    "            gw_base_avg += gw_base / n_task\n",
    "            \n",
    "            # display inside epoch\n",
    "            if (iter_i + 1) % disp_round == 0:\n",
    "                if (iter_i + 1) == disp_round: print(\"\\n\")\n",
    "                print (\"iteration:\", iter_i + 1, end='\\t')\n",
    "                # print (loss_train, reg_loss, loss_valid, loss_test)\n",
    "                print (round(accu_train_meta/(0+1), 3), round(accu_query_meta/(0+1), 3), round(accu_train_base/(0+1), 3), round(accu_query_base/(0+1), 3))\n",
    "        \n",
    "        learning_rate = learn_rate_beta if is_train else learn_rate_alpha\n",
    "        w_meta += learning_rate * gw_meta_avg\n",
    "        w_base += amplifier * learning_rate * gw_base_avg\n",
    "    \n",
    "    if not is_train:\n",
    "        w_meta = w_copy\n",
    "        \n",
    "    return accu_query_meta, accu_query_base\n",
    "\n",
    "\n",
    "def testProcedure():\n",
    "\n",
    "    print(\"intermediate testing...\")\n",
    "    print(time.asctime( time.localtime(time.time()) ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    max_sample = 20\n",
    "    accu_meta_avg, accu_base_avg = 0, 0\n",
    "\n",
    "    for i in range(max_sample):\n",
    "        accu_meta, accu_base = oneEpisode(is_meta=False, is_train=False)\n",
    "        accu_meta_avg += accu_meta / max_sample\n",
    "        accu_base_avg += accu_base / max_sample\n",
    "        if (i+1)%10==0:\n",
    "            print('Sample (test):', i+1, '\\tAccuracy:', round(accu_meta, 3), round(accu_base, 3))\n",
    "            print(time.asctime( time.localtime(time.time()) ))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    print(\"[test]: Average accuracy:\", accu_meta_avg, accu_base_avg)\n",
    "    print(\"\\n\")\n",
    "    return accu_meta_avg, accu_base_avg\n",
    "\n",
    "\n",
    "k_shots = 1\n",
    "num_task = 2\n",
    "batch_size = 2*k_shots     # mini batch size for training\n",
    "max_epsd = 200\n",
    "learn_rate_alpha = 5e-9    # meta update step size alpha\n",
    "learn_rate_beta = 5e-9    # meta update step size beta\n",
    "amplifier = 10    # indicate larger step size for baseline finetuning, should be 1 in fact ?\n",
    "\n",
    "n_step_tuning = 16      # finetuning steps\n",
    "disp_round = 4\n",
    "\n",
    "w_meta = np.zeros((d+1,1))\n",
    "loss_record = [[],[],[]]\n",
    "accu_record = [[],[],[]]\n",
    "\n",
    "\n",
    "print(\"Begin training...\")\n",
    "print(time.asctime( time.localtime(time.time()) ))\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in range(max_epsd):\n",
    "    accu_meta, _ = oneEpisode(is_meta=True, is_train=True)\n",
    "    if (i+1)%100==0:\n",
    "        print('Epoch (train):', i+1, '\\tAccuracy:', round(accu_meta, 3))\n",
    "        print(time.asctime( time.localtime(time.time()) ))\n",
    "        print(np.sum(np.square(w_meta)))\n",
    "        print(\"\\n\")\n",
    "    if (i+1)%1000==0:\n",
    "        accu_meta, accu_base = testProcedure()\n",
    "        accu_record[0].append(accu_meta)\n",
    "        accu_record[1].append(accu_base)\n",
    "\n",
    "\n",
    "plt.plot(accu_record[0],'b-',label='meta')\n",
    "plt.plot(accu_record[1],'r-',label='baseline')\n",
    "plt.xlabel('training episode (x1000)')\n",
    "plt.ylabel('accuracy')\n",
    "#plt.ylim([0,10])\n",
    "plt.title('Binary classification (k-shots) accuracy on MNIST, k = ' + str(k_shots))\n",
    "plt.legend()\n",
    "plt.savefig('../figure/'+str(k_shots)+'_shots_'+str(max_epsd)+'_epsd_'+str(num_task)+'_tasks_'+str(n_step_tuning)+'_steps.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
